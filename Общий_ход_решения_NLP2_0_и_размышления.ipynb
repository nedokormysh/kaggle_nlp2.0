{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQUqZ8iMQ2xCoyY/f7+0Sz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nedokormysh/kaggle_nlp2.0/blob/main/%D0%9E%D0%B1%D1%89%D0%B8%D0%B9_%D1%85%D0%BE%D0%B4_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F_NLP2_0_%D0%B8_%D1%80%D0%B0%D0%B7%D0%BC%D1%8B%D1%88%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка библиотек"
      ],
      "metadata": {
        "id": "mMyEla_QAtsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vcHw55_-7hu"
      },
      "outputs": [],
      "source": [
        "!pip install optuna -q\n",
        "!pip install catboost -q\n",
        "!pip install natasha -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor, early_stopping, Dataset, LGBMClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from mlxtend.feature_selection import ColumnSelector\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import CatBoostPruningCallback\n",
        "\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import gensim.downloader\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.fasttext import FastText\n",
        "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger"
      ],
      "metadata": {
        "id": "LyR362EGAzGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# фиксируем значения\n",
        "RND_STATE = 7575\n",
        "N_EST = 1000\n",
        "sampler = optuna.samplers.TPESampler(seed=RND_STATE)"
      ],
      "metadata": {
        "id": "K_GoS-s8A7AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Этапы обработки и загрузки данных."
      ],
      "metadata": {
        "id": "u9wZMC_0BBBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ВНИМАНИЕ:** данный ноутбук составлялся для демонстрации общего хода построения моделей. В разное время использовались разные данные, и по-разному составлялись модели. Возможно сейчас запуск всех ячеек этого ноутбука вообще не даст результата. Необходимо загружать и запускать уже конкретные варианты ноутбуков. Если потребуется продемострировать именно конкретный вариант или объяснить какой-то код, то готов это сделать."
      ],
      "metadata": {
        "id": "g0VBqa7LKaw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Было несколько вариантов парсинга данных. И несколько этапов. Полученные данные сохранял и загружал из гугла диска.\n",
        "Какие-то последние варианты моделей и данных лежат здесь: https://drive.google.com/drive/folders/1u_2ejLOI2kMkrhoXdVYkxkKWwSQVUWlE?usp=sharing\n"
      ],
      "metadata": {
        "id": "UZ_Ps8PyBEhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1\n",
        "\n",
        "Первый вариант был достаточно прост: у меня был уже написан парсинг для сайта фонтанка https://github.com/nedokormysh/DP_NLP_bootcamp/tree/parsing .\n",
        "\n",
        "Достаточно очевидно, что этого сайта было недостаточно. И требовалось спарсить лента.ру. Вариант парсинга ленты нашёл на kaggle. https://github.com/nedokormysh/kaggle_nlp2.0/blob/main/notebooks/parsing_news_from_lenta_ru_.ipynb\n",
        "Насколько помню, то изначально парсил с 2020-01-01 по 2023-10-21.\n",
        "\n",
        "Было около 26000 записей с фонтанки и около 30000 записей с ленты. Там были дубли и разные хвосты. Поэтому потом привожу код ниже - обработал их для сборки в единый датасет.\n",
        "В принципе и всё. После этого просто обучил модель catboost, в качестве текстовых признаков указал признак content. Получил результат и заслал на платформу. Это дало отличный результат. Модель сохранил."
      ],
      "metadata": {
        "id": "yoG_CeSaB7ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AO3egSU2BD14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сборка общего датасета."
      ],
      "metadata": {
        "id": "AbcJRQkfJAP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fontanka"
      ],
      "metadata": {
        "id": "90axKlx-Crx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь просто загружал данные, которые спарсил изначально"
      ],
      "metadata": {
        "id": "s6ZFQo_HDS5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_fontanka = pd.DataFrame()\n",
        "\n",
        "for i in range(1, 13):\n",
        "    df_ = pd.read_csv(f'https://raw.githubusercontent.com/nedokormysh/DP_NLP_bootcamp/parsing/data/fontanka_full_{i}.csv')\n",
        "    df_fontanka = pd.concat([df_fontanka, df_])"
      ],
      "metadata": {
        "id": "GaO3O-PqCubp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаляем дубли\n",
        "df_fontanka.drop_duplicates(inplace=True)\n",
        "# Удаляем пропуски\n",
        "df_fontanka.dropna(inplace=True)\n",
        "# Переводим тэги к нижнему регистру\n",
        "df_fontanka['topic']= df_fontanka['topic'].apply(lambda x: x.lower())\n",
        "\n",
        "df_fontanka.shape"
      ],
      "metadata": {
        "id": "VjbL51-EIv7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fontanka = df_fontanka[df_fontanka['topic'].isin(['общество',\n",
        "                                                    #  'город',\n",
        "                                                    #  'власть',\n",
        "                                                    #  'политика',\n",
        "                                                     'финансы',\n",
        "                                                    #  'новости компаний',\n",
        "                                                     'спорт',\n",
        "                                                     'строительство',\n",
        "                                                     'недвижимость',\n",
        "                                                     'туризм',\n",
        "                                                     'технологии',\n",
        "                                                    #  'образ жизни'\n",
        "                                                     ])]"
      ],
      "metadata": {
        "id": "AxSB17tTJMTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "di_fontanka = {'общество' : 0,\n",
        "               'финансы' : 1,\n",
        "               'спорт' : 4,\n",
        "               'строительство' : 6,\n",
        "               'недвижимость': 6,\n",
        "               'туризм' : 7,\n",
        "               'технологии' : 8}"
      ],
      "metadata": {
        "id": "ICtcWDNyJP9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_lst_f = [\"url\", \"datetime\", \"views\", 'title']\n",
        "\n",
        "df_fontanka.drop(drop_lst_f, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "fZRcM-9vJVEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lenta"
      ],
      "metadata": {
        "id": "0GspuX_1JaMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Изначально не пользовался гугл диском, а загружал в колаб напрямую."
      ],
      "metadata": {
        "id": "R581SUDHJeHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tbl = pd.read_excel('/content/drive/MyDrive/competitions/NLP2.0/data/parsing_data/lenta2019-01-01_2023-12-21.xlsx')\n",
        "df_tbl.head()"
      ],
      "metadata": {
        "id": "PU6fdJSlJb7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь просто просмотрел разные статьи с ленты, чтобы верно указать код топика."
      ],
      "metadata": {
        "id": "QlgifowUJpj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # category = {1: \"общество\",\n",
        "#                  2: \"мир\",\n",
        "#                  3: \"former_ussr\",\n",
        "#                  4: \"economy\",\n",
        "#                  5: 'наука и техника',\n",
        "#                  6: 'культура',\n",
        "#                  7: 'интернет и сми',\n",
        "#                  8: 'спорт',\n",
        "#                  9: 'из жизни',\n",
        "#                  12: 'среда обитания',\n",
        "#                  37: 'силовые структуры',\n",
        "#                  40: 'бизнес',\n",
        "#                  47: 'ценности',\n",
        "#                  48: 'путешествия',\n",
        "#                  49:'69-парралелль',\n",
        "#                  53:'нацпроекты',\n",
        "#                  86:'моя страна',\n",
        "#                  49:'забота о себе'}"
      ],
      "metadata": {
        "id": "b2ItpK8xJ4eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "di_lenta_tmp= {1: 'общество',\n",
        "               3: 'бывший_СССР',\n",
        "               4: 'экономика',\n",
        "               5: 'технологии',\n",
        "               8: 'спорт',\n",
        "               37: 'силовые_структуры',\n",
        "               48: 'путешествия',\n",
        "               49: 'забота_о_себе'}"
      ],
      "metadata": {
        "id": "0FNeopaLJn56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tbl['topic'] = pd.Series(df_tbl.bloc.map(di_lenta_tmp))"
      ],
      "metadata": {
        "id": "p11pAaEyKErx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lenta = df_tbl[df_tbl['topic'].isin(['общество',\n",
        "                                        'бывший_СССР',\n",
        "                                        'экономика',\n",
        "                                        'силовые_структуры',\n",
        "                                        'технологии',\n",
        "                                        'спорт',\n",
        "                                        'путешествия',\n",
        "                                        'забота_о_себе'])]"
      ],
      "metadata": {
        "id": "16tkCCWUKGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_lst = [\"docid\", \"modified\", \"lastmodtime\", \"type\", \"domain\", \"status\",\n",
        "            \"part\", \"tags\", \"image_url\", \"rightcol\", \"bloc\", \"pubdate\", \"url\",\n",
        "            \"Unnamed: 0\", \"snippet\",\n",
        "            'title']\n",
        "\n",
        "df_lenta.drop(drop_lst, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "T-OAhtaHKU0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "di_lenta= {'общество': 0,\n",
        "           'экономика': 1,\n",
        "           'силовые_структуры': 2,\n",
        "           'бывший_СССР': 3,\n",
        "           'спорт': 4,\n",
        "           'забота_о_себе': 5,\n",
        "           'путешествия': 7,\n",
        "           'технологии': 8}"
      ],
      "metadata": {
        "id": "BWg33HfMKX9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lenta['topic'] = pd.Series(df_lenta.topic.map(di_lenta))"
      ],
      "metadata": {
        "id": "bJG-PuB3LeR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_lenta.rename(columns={\"text\": \"content\"}, inplace=True)"
      ],
      "metadata": {
        "id": "DHjqZHlqLx9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([df_fontanka,\n",
        "#                     # df_ria,\n",
        "                    df_lenta])"
      ],
      "metadata": {
        "id": "P6N0dY7oLz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.to_csv('df_5_fontanka_lenta.csv', index=False)"
      ],
      "metadata": {
        "id": "ervMvfxVL2h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2\n",
        "\n",
        "Изначально показалось, что необходимо добавить статьи РИА. Это был второй подход к задаче. Я очень много времени потратил на сбор данных с риа. Множество раз не получилось их собрать. Почему-то я верил, что можно запустить один раз и долго подождать, а не собирать кусками данные. (код ноутбука здесь https://github.com/nedokormysh/kaggle_nlp2.0/blob/main/notebooks/Копия_блокнота__ria_parsing_task_ipynb_.ipynb) Это оказалось ошибочной стратегией. В конечном счёте, оказался работать в колабе и на основе приведённого ноутбука на локальном компе добавил пару строк, чтобы скачивать данные через каждые 100 итераций. Это позволило собрать данные. Впрочем, это только всё ухудшило. И видимо данные с риа новостей собирать не следовало. Но было уже очень обидно их не использовать."
      ],
      "metadata": {
        "id": "Kll8iS6xLtyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### РИА"
      ],
      "metadata": {
        "id": "PlnI5ak3NJfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ria = pd.read_csv('/content/drive/MyDrive/competitions/NLP2.0/tmp_data/df_ria.csv')\n",
        "df_ria_2 = pd.read_csv('/content/drive/MyDrive/competitions/NLP2.0/tmp_data/df_ria_2.csv')\n",
        "df_ria2 = pd.read_csv('/content/drive/MyDrive/competitions/NLP2.0/tmp_data/df_ria2.csv')\n",
        "df_ria_0 = pd.read_csv('/content/drive/MyDrive/competitions/NLP2.0/tmp_data/df_ria_0.csv')"
      ],
      "metadata": {
        "id": "YMokoK69NF5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ria = pd.concat([df_ria, df_ria_2, df_ria2, df_ria_0])"
      ],
      "metadata": {
        "id": "rOVSjNSiN4AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ria.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "PVEEYbOiN5mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_lst = [\"id\", \"url\", \"title\", \"subtitle\", \"datetime\"]\n",
        "\n",
        "df_ria.drop(drop_lst, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5hFlBS57N7Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "di_ria = {'society': 0,\n",
        "          'economy': 1,\n",
        "          'science': 8}"
      ],
      "metadata": {
        "id": "QaTjM2C1N9Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ria['topic'] = pd.Series(df_ria.topic.map(di_ria))"
      ],
      "metadata": {
        "id": "160Dzy-4N_y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3\n",
        "Собственно изначальная модель насколько помню была просто обучена даже не на всём объёме данных, а только на X_train."
      ],
      "metadata": {
        "id": "5uTlfO7zOlMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Получение разбиения."
      ],
      "metadata": {
        "id": "xcwhYNmOO3ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузка данных\n",
        "trainw2v = pd.read_csv(paths.path_prep_train_no_ria)\n",
        "test = pd.read_csv(paths.path_prep_test)"
      ],
      "metadata": {
        "id": "5xcR38BEOpcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Данные оптимизировал по объёму\n",
        "\n",
        "\n",
        "# def reduce_mem_usage(df):\n",
        "#     \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "#         to reduce memory usage.\n",
        "#     \"\"\"\n",
        "#     start_mem = df.memory_usage().sum() / 1024**2\n",
        "#     print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "#     for col in df.columns:\n",
        "#         col_type = df[col].dtype\n",
        "\n",
        "#         if col_type != object:\n",
        "#             c_min = df[col].min()\n",
        "#             c_max = df[col].max()\n",
        "#             if str(col_type)[:3] == 'int':\n",
        "#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "#                     df[col] = df[col].astype(np.int8)\n",
        "#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "#                     df[col] = df[col].astype(np.int16)\n",
        "#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "#                     df[col] = df[col].astype(np.int32)\n",
        "#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "#                     df[col] = df[col].astype(np.int64)\n",
        "#             else:\n",
        "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "#                     df[col] = df[col].astype(np.float16)\n",
        "#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "#                     df[col] = df[col].astype(np.float32)\n",
        "#                 else:\n",
        "#                     df[col] = df[col].astype(np.float64)\n",
        "#         # else:\n",
        "#         #     df[col] = df[col].astype('category')\n",
        "\n",
        "#     end_mem = df.memory_usage().sum() / 1024**2\n",
        "#     print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "#     print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "#     return df\n",
        "\n",
        "# def import_data(file):\n",
        "#     \"\"\"create a dataframe and optimize its memory usage\"\"\"\n",
        "#     df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n",
        "#     df = reduce_mem_usage(df)\n",
        "#     return df\n",
        "\n",
        "\n",
        "# print('train')\n",
        "# train = import_data(paths.path_train)\n",
        "\n",
        "# print('-' * 80)\n",
        "# print('test')\n",
        "# test = import_data(paths.path_test)"
      ],
      "metadata": {
        "id": "k93OASzPPCQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаляем дубли\n",
        "trainw2v.drop_duplicates(inplace=True)\n",
        "# Удаляем пропуски\n",
        "trainw2v.dropna(inplace=True)\n",
        "\n",
        "print(f'train shape = {trainw2v.shape},  test shape = {test.shape}')\n",
        "print(f'train, test is null: {trainw2v.isna().any().any(), test.isna().any().any()}')"
      ],
      "metadata": {
        "id": "l_Zzlh9PPLh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = ['content',\n",
        "                #  'content_clean' в исходных данных колонка content_clean появилась позже\n",
        "                 ]\n",
        "# cat_features_indxs = [1]\n",
        "targets = ['topic']\n",
        "# cat_cols = ['content', 'content_clean']\n",
        "\n",
        "filtered_features = [i for i in trainw2v.columns if (i not in targets)]"
      ],
      "metadata": {
        "id": "jz1K_IY7PNNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = trainw2v[filtered_features].drop(targets, axis=1, errors=\"ignore\")\n",
        "y = trainw2v[\"topic\"]"
      ],
      "metadata": {
        "id": "5AiOWsGWPYOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиение на обучающую и валидационную выборки\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RND_STATE)"
      ],
      "metadata": {
        "id": "lJDK6GH9PaHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собственно на этом X_train и была построена первая модель."
      ],
      "metadata": {
        "id": "mgmZYN9PPcno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4\n",
        "\n",
        "Одновременно с загрузкой риа, пытался при помощи optuna подобрать гиперпараметры для базовой модели."
      ],
      "metadata": {
        "id": "m3MbMn0YOAXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optuna для catboost"
      ],
      "metadata": {
        "id": "TGxkzimNOkO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_catboost(trial, train, val):\n",
        "    X_train, y_train = train\n",
        "    X_val, y_val = val\n",
        "\n",
        "    param = {\n",
        "        'iterations' : 500,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.04, step=0.001),\n",
        "        \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 30, 70, step=10),\n",
        "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.3, 0.4, step=0.1),\n",
        "        \"auto_class_weights\": trial.suggest_categorical(\"auto_class_weights\",\n",
        "                                                        [#\"SqrtBalanced\",\n",
        "                                                         #\"Balanced\",\n",
        "                                                         \"None\"]),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 9, 10),\n",
        "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [#\"Ordered\",\n",
        "                                                                     \"Plain\"]),\n",
        "        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\",\n",
        "                                                              [#\"Bayesian\",\n",
        "                                                               \"Bernoulli\",\n",
        "                                                               #\"MVS\"\n",
        "                                                               ]),\n",
        "        \"used_ram_limit\": \"10gb\",\n",
        "        \"eval_metric\": \"Accuracy\", # Тоже стоит заранее определиться\n",
        "        # 'loss_function': 'MultiClass'\n",
        "    }\n",
        "\n",
        "\n",
        "    if param[\"bootstrap_type\"] == \"Bayesian\":\n",
        "        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n",
        "\n",
        "    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n",
        "        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n",
        "\n",
        "\n",
        "    clf = CatBoostClassifier(\n",
        "        **param,\n",
        "        thread_count=-1,\n",
        "        random_seed=RND_STATE,\n",
        "        text_features=text_features)\n",
        "    #  Создаем объект callback\n",
        "    pruning_callback = CatBoostPruningCallback(trial, \"Accuracy\")\n",
        "\n",
        "    clf.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        eval_set=(X_val, y_val),\n",
        "        verbose=0,\n",
        "        plot=False,\n",
        "        early_stopping_rounds=5,\n",
        "        callbacks=[pruning_callback],\n",
        "    )\n",
        "\n",
        "    # запускаем процесс прунинга\n",
        "    pruning_callback.check_pruned()\n",
        "\n",
        "    y_pred = clf.predict(X_val)\n",
        "    return clf, y_pred"
      ],
      "metadata": {
        "id": "JKd5qrgyPntQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial, return_models=False):\n",
        "    n_splits = 3\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RND_STATE)\n",
        "\n",
        "    scores_simple_acc, scores_balanced_acc, models = [], [], []\n",
        "\n",
        "    for train_idx, valid_idx in kf.split(X_train):\n",
        "        train_data = X_train.iloc[train_idx, :], y_train.iloc[train_idx]\n",
        "        valid_data = X_train.iloc[valid_idx, :], y_train.iloc[valid_idx]\n",
        "\n",
        "        # Подаем trials для перебора\n",
        "        model, y_pred = fit_catboost(trial, train_data, valid_data) # Определили выше\n",
        "\n",
        "        # scores_simple_acc.append(accuracy_score(y_pred, valid_data[1]))\n",
        "        scores_balanced_acc.append(balanced_accuracy_score(y_pred, valid_data[1]))\n",
        "        models.append(model)\n",
        "        break\n",
        "\n",
        "    result = np.mean(scores_balanced_acc)\n",
        "    # result = scores_balanced_acc\n",
        "    print(f'result = {result}')\n",
        "\n",
        "    if return_models:\n",
        "        return result, models\n",
        "    else:\n",
        "        return result"
      ],
      "metadata": {
        "id": "1xHzlbp7Ps7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.integration import CatBoostPruningCallback, LightGBMPruningCallback, XGBoostPruningCallback\n",
        "from optuna.integration.pytorch_lightning import PyTorchLightningPruningCallback\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "# optuna.logging.set_verbosity(optuna.logging.ERROR)"
      ],
      "metadata": {
        "id": "KNpidaeaPvAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\",\n",
        "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "                            study_name=\"Catboost_clf_simple\",\n",
        "                            sampler=sampler)\n",
        "study.optimize(objective,\n",
        "               n_trials=40,\n",
        "               n_jobs=-1,\n",
        "               show_progress_bar=True)"
      ],
      "metadata": {
        "id": "2wxz3pGgPw2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "print(\"  Params: \")\n",
        "\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ],
      "metadata": {
        "id": "CYMrmTxrP6QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# История изменения от числа испытаний\n",
        "optuna.visualization.plot_optimization_history(study)"
      ],
      "metadata": {
        "id": "7kZHdKcCP9Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = ['learning_rate',\n",
        "          'l2_leaf_reg',\n",
        "          'colsample_bylevel',\n",
        "          'auto_class_weights',\n",
        "          'depth',\n",
        "          'bootstrap_type',\n",
        "          'boosting_type']\n",
        "\n",
        "optuna.visualization.plot_slice(study, params=params, target_name='accuracy')"
      ],
      "metadata": {
        "id": "9xAYGA4HP_u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это заняло огромное количество времени. Вообще optuna для вот базовой модели делал только на части X_train. Брал какое-то количество записей. И это оказалось абсолютно бесполезно. Ничем не улучшило качество."
      ],
      "metadata": {
        "id": "0gvT9HvoQCHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# В дальнейшем, потребовалось в разные модели подавать разные колонки, поэтому был написан миникласс\n",
        "\n",
        "class CustomSelectColumns():\n",
        "    '''\n",
        "    .. notes::\n",
        "    Класс для вытаскивания из датасета только требуемых колонок\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 fil_feats: list[str]=None, clean: bool=False):\n",
        "        self.fil_feats = fil_feats\n",
        "        self.clean = clean\n",
        "\n",
        "        # print('Init done')\n",
        "    def fit(self, X: pd.DataFrame, y: pd.Series=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: pd.DataFrame, y: pd.Series=None) -> pd.DataFrame:\n",
        "        X_ = X.copy()\n",
        "        X_filtered = X_[self.fil_feats]\n",
        "        X_filtered = pd.DataFrame(X_filtered)\n",
        "        if self.clean:\n",
        "            return X_filtered[self.fil_feats[0]].str.split()\n",
        "        else:\n",
        "            return pd.DataFrame(X_filtered)"
      ],
      "metadata": {
        "id": "Qd4j9Lw1QaWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# более старый вариант выбора колонок, здесь можно возвратить именно строки, а не просто датасет\n",
        "\n",
        "# class CustomColumns():\n",
        "#     '''\n",
        "#     .. notes::\n",
        "#     Класс для вытаскивания из датасета только требуемых колонок\n",
        "#     '''\n",
        "#     def __init__(self,\n",
        "#                  fil_feats: list[str]=None, clean: bool=False):\n",
        "#         # self.fil_feats = fil_feats\n",
        "#         self.clean = clean\n",
        "\n",
        "#         # print('Init done')\n",
        "#     def fit(self, X: pd.DataFrame, y: pd.Series=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X: pd.DataFrame, y: pd.Series=None) -> pd.DataFrame:\n",
        "#         X_ = X.copy()\n",
        "#         X_filtered = X_[self.fil_feats]\n",
        "#         X_filtered = pd.DataFrame(X_filtered)\n",
        "#         if self.clean:\n",
        "#             return X_filtered[self.fil_feats[0]].str.split()\n",
        "#         else:\n",
        "#             return pd.DataFrame(X_filtered)"
      ],
      "metadata": {
        "id": "PquzijVgXCnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Catboost_pipe = Pipeline([\n",
        "    ('col_selector', CustomSelectColumns(['content'])),\n",
        "    ('Catboost_simple', CatBoostClassifier(eval_metric='Accuracy',\n",
        "                           iterations=200,\n",
        "                           thread_count=-1,\n",
        "                           random_seed=RND_STATE,\n",
        "                           text_features=text_features))\n",
        "    ])\n",
        "\n",
        "# train_dataset = Pool(data=X_train, label=y_train, text_features=['content'])\n",
        "# eval_dataset = Pool(data=X_val, label=y_val, text_features=['content'])\n",
        "\n",
        "Catboost_pipe.fit(X_train, y_train,\n",
        "                  Catboost_simple__text_features=['content'],\n",
        "                  Catboost_simple__verbose=5,\n",
        "                  Catboost_simple__early_stopping_rounds=10,\n",
        "                  Catboost_simple__eval_set=(X_val[['content']], y_val)\n",
        "                  )"
      ],
      "metadata": {
        "id": "aCpjXs_BQnqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Это самый первый вариант запуска и построения модели. Здесь не используются pipeline.\n",
        "# Зато используются Pool библиотеки catboost для ускорения работы\n",
        "\n",
        "# %%time\n",
        "# clf_1 = CatBoostClassifier(eval_metric= 'Accuracy',\n",
        "#                            iterations=200,\n",
        "#                            thread_count=-1,\n",
        "#                            random_seed=RND_STATE,\n",
        "#                            text_features=text_features)\n",
        "\n",
        "# train_dataset = Pool(data=X_train, label=y_train, text_features=['content'])\n",
        "# eval_dataset = Pool(data=X_val, label=y_val, text_features=['content'])\n",
        "\n",
        "# clf_1.fit(train_dataset,\n",
        "#         eval_set=eval_dataset,\n",
        "#         verbose=50,\n",
        "#         early_stopping_rounds=200)"
      ],
      "metadata": {
        "id": "2QxTbjCcQszq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5\n",
        "Одновременно шла работа для создания данных для других моделей. Для этого использовались навыки, полученные в ходе обучения. И была создана доп. колонка content_clean. при помощи библиотеки natasha"
      ],
      "metadata": {
        "id": "v9MjQc2PQUHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)"
      ],
      "metadata": {
        "id": "yMvyC39EQTjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('russian')\n",
        "stop_words.extend(['что', 'это', 'так',\n",
        "                    'вот', 'быть', 'как',\n",
        "                    'в', '—', 'к', 'за', 'из', 'из-за',\n",
        "                    'на', 'ок', 'кстати',\n",
        "                    'который', 'мочь', 'весь',\n",
        "                    'еще', 'также', 'свой',\n",
        "                    'ещё', 'самый', 'ул', 'комментарий',\n",
        "                    'английский', 'язык'])"
      ],
      "metadata": {
        "id": "XooCJ5YZRk62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# функция для обработки текстов\n",
        "\n",
        "def text_prep(text) -> str:\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "\n",
        "    lemmas = [_.lemma for _ in doc.tokens]\n",
        "    words = [lemma for lemma in lemmas if lemma.isalpha() and len(lemma) > 2]\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered_words)"
      ],
      "metadata": {
        "id": "WAg6RkC4RnVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Удаляем дубли\n",
        "# train.drop_duplicates(inplace=True)\n",
        "# # Удаляем пропуски\n",
        "# train.dropna(inplace=True)\n",
        "\n",
        "# print(f'train shape = {train.shape},  test shape = {test.shape}')\n",
        "# print(f'train, test is null: {train.isna().any().any(), test.isna().any().any()}')"
      ],
      "metadata": {
        "id": "HEo3vpgGRvAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train['content_clean'] = train.content.apply(text_prep)"
      ],
      "metadata": {
        "id": "sbx4DZ-nRwx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.to_csv('train_02.01_prep.csv', index=False)"
      ],
      "metadata": {
        "id": "-aqTecdFRyFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "test['content_clean'] = test.content.apply(text_prep)"
      ],
      "metadata": {
        "id": "B7DHn8U3R0ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соответственно в дальнейшем в данных уже всегда присутствовала эта колонка.\n",
        "\n",
        "Хотя запускалось всё множесто раз. Потому что после сборки данных с риа и получения плохих результатов, было решено увеличить количество новостей с лента.ру. И несколько раз изначальный датасет пересобирался, и, следовательно, и все шаги по обработке данных проводились заново."
      ],
      "metadata": {
        "id": "l82cl8p8SB30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6\n",
        "Векторизация слов. Изначально для других моделей выбрал стандартную word2vec. В принципе, долгое время думал, что нужно стараться быть более близким к пройдённому на курсе.\n",
        "Это сыграло плохую шутку - я пропустил последнюю лекцию, где показывались предобученные модели. И слишком долго бился только в стандартные методы.\n",
        "\n",
        "Для векторизации слов использовал word2vec. fasttext - не давал никаких улучшений. поэтому от него отказался.\n",
        "\n",
        "Для вектора предложений использовал tfidf. Впрочем потом и обычное усреднение тоже было использовано. Всё это было напрасно."
      ],
      "metadata": {
        "id": "cK9DLkjMTLby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TFIDF"
      ],
      "metadata": {
        "id": "FRKeGepkTDS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Для получения вектора предложения используем tf_idf\n",
        "class TfidfEmbeddingVectorizer(object):\n",
        "    \"\"\"Get tfidf weighted vectors\"\"\"\n",
        "    def __init__(self, model):\n",
        "        self.word2vec = model.wv\n",
        "        self.word2weight = None\n",
        "        self.dim = model.vector_size\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        max_idf = max(tfidf.idf_)\n",
        "        self.word2weight = defaultdict(\n",
        "            lambda: max_idf,\n",
        "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.word2vec.get_vector(w) * self.word2weight[w]\n",
        "                         for w in words if w in self.word2vec] or\n",
        "                        [np.zeros(self.dim)], axis=0)\n",
        "                for words in X\n",
        "            ])"
      ],
      "metadata": {
        "id": "O3oVfPO5TFtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2vec"
      ],
      "metadata": {
        "id": "HB16tlf0TGS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train, X_val, y_train, y_val = train_test_split(X.content_clean.str.split(),\n",
        "#                                                   y,#.values,\n",
        "#                                                   test_size=0.2,\n",
        "#                                                   random_state=RND_STATE)\n",
        "\n",
        "\n",
        "model_w2v = Word2Vec(sentences=X_train,\n",
        "                 vector_size=300,\n",
        "                 min_count=5,\n",
        "                 window=25,\n",
        "                 seed=RND_STATE)"
      ],
      "metadata": {
        "id": "PU5pzi6eU8N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'model_w2v.pkl', \"wb\") as f:\n",
        "        pickle.dump(model_w2v, f)"
      ],
      "metadata": {
        "id": "XnUUjCU2U-_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"/content/drive/MyDrive/competitions/NLP2.0/models/model_w2v.pkl\", \"rb\") as f:\n",
        "        model_w2v = pickle.load(f)"
      ],
      "metadata": {
        "id": "tvqj6PinUcjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple models"
      ],
      "metadata": {
        "id": "E9VVPnXTVDo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## log reg"
      ],
      "metadata": {
        "id": "empvVmGEVUxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соответственно в первом варианте, построения логистической регрессии даже был немного другой класс для выбора колонок. И там возвращались именно строки. Которые подавались по pipeline в tfidf и scaler.\n",
        "\n",
        "В дальнейшем создал отдельные колонки в датасетах, в которых были усреднённые значения векторов, tfidf значения. И уже и построения моделей было немного другим."
      ],
      "metadata": {
        "id": "uslNW8wwVY7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline variant\n",
        "%%time\n",
        "pipe_log = Pipeline([('col_selector', CustomSelectColumns(['content_clean'], clean=True)),\n",
        "                 ('w2v_tfidf', TfidfEmbeddingVectorizer(model_w2v)),\n",
        "                 ('scaler', StandardScaler()),\n",
        "                 ('clf', LogisticRegression(random_state=RND_STATE,\n",
        "                                            max_iter=10000,\n",
        "                                            # class_weight='balanced'\n",
        "                                            ))])\n",
        "\n",
        "pipe_log.fit(X_train, y_train)\n",
        "\n",
        "print(classification_report(y_val, pipe_log.predict(X_val)))"
      ],
      "metadata": {
        "id": "yurQRKcQVHGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_log_reg(trial, train, val):\n",
        "    X_train, y_train = train\n",
        "    X_val, y_val = val\n",
        "\n",
        "    param = {\"dual\": False,\n",
        "             \"C\": trial.suggest_float(\"C\", 2.5, 3, step=0.1),\n",
        "             \"solver\": trial.suggest_categorical(\"solver\",\n",
        "                                                 [\"sag\",\n",
        "                                                  #\"saga\",\n",
        "                                                  \"lbfgs\"\n",
        "                                                  ]),\n",
        "             \"random_state\": RND_STATE,\n",
        "             \"max_iter\": trial.suggest_int(\"max_iter\", 5500, 6300, step=200),\n",
        "             'multi_class': trial.suggest_categorical(\"multi_class\",\n",
        "                                                      [#'auto',\n",
        "                                                       'ovr',\n",
        "                                                       #'multinomial'\n",
        "                                                       ]),\n",
        "             'n_jobs': -1,\n",
        "             'verbose': 0\n",
        "    }\n",
        "\n",
        "    # print(f'param[\"solver\"] = {param[\"solver\"]}')\n",
        "    if param[\"solver\"] == \"saga\":\n",
        "        param[\"penalty\"] = trial.suggest_categorical(\"penalty\",\n",
        "                                             ['l1',\n",
        "                                              'l2', #'elasticnet',\n",
        "                                              None])\n",
        "    else:\n",
        "         param[\"penalty\"] = trial.suggest_categorical(\"penalty\",\n",
        "                                             ['l2',\n",
        "                                              #None\n",
        "                                              ])\n",
        "\n",
        "    clf = LogisticRegression(**param)\n",
        "\n",
        "    pipe = Pipeline([('w2v', TfidfEmbeddingVectorizer(model_w2v)),\n",
        "                    ('scaler', StandardScaler()),\n",
        "                    ('clf', clf)])\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = pipe.predict(X_val)\n",
        "    return pipe, y_pred"
      ],
      "metadata": {
        "id": "lcF0ygVdWFnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_log(trial, return_models=False):\n",
        "    n_splits = 3\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RND_STATE)\n",
        "\n",
        "    scores_simple_acc, scores_balanced_acc, models = [], [], []\n",
        "\n",
        "    for train_idx, valid_idx in kf.split(X_train.index):\n",
        "        train_data = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
        "        valid_data = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n",
        "\n",
        "        # Подаем trials для перебора\n",
        "        model, y_pred = fit_log_reg(trial, train_data, valid_data)\n",
        "\n",
        "        # scores_simple_acc.append(accuracy_score(y_pred, valid_data[1]))\n",
        "        scores_balanced_acc.append(balanced_accuracy_score(y_pred, valid_data[1]))\n",
        "        models.append(model)\n",
        "        break\n",
        "\n",
        "    result = np.mean(scores_balanced_acc)\n",
        "    # print(f'result = {result}')\n",
        "\n",
        "    if return_models:\n",
        "        return result, models\n",
        "    else:\n",
        "        return result"
      ],
      "metadata": {
        "id": "2eH9MsncWHhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction=\"maximize\",\n",
        "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "                            sampler=sampler,\n",
        "                            study_name=\"Log_Reg_clf\")\n",
        "study.optimize(objective_log,\n",
        "               n_trials=10,\n",
        "               n_jobs=-1,\n",
        "               show_progress_bar=True)"
      ],
      "metadata": {
        "id": "rvZIDWmQW7ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все модели оптимизировал optuna - всё бесполезно."
      ],
      "metadata": {
        "id": "q-sYVtDXXPoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "w3nA801BXW32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pipeline version\n",
        "%%time\n",
        "pipe_svc = Pipeline([('col_selector', CustomSelectColumns(['content_clean'], clean=True)),\n",
        "                     ('w2v_tfidf', TfidfEmbeddingVectorizer(model_w2v)),\n",
        "                     ('scaler', StandardScaler()),\n",
        "                     ('clf', SVC(random_state=RND_STATE,\n",
        "                             max_iter=10000,\n",
        "                            #  class_weight='balanced'\n",
        "                             ))])\n",
        "\n",
        "pipe_svc.fit(X_train, y_train)\n",
        "\n",
        "print(classification_report(y_val, pipe_svc.predict(X_val)))"
      ],
      "metadata": {
        "id": "NHWVdD38XYTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stack"
      ],
      "metadata": {
        "id": "Bo0UAYvOXd1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соответственно, когда обучил 3 модели: catboost, svm, log reg - сделал стекинг моделей. И это, в принципе дало мне наилучший результат. Впрочем, я его не использовал. И всё равно это давало улучшения на доли процентов. Причём это улучшение было именно уже на private leaderboard/"
      ],
      "metadata": {
        "id": "0BUrVBQGXgqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Некоторые модели для построение ансамбля\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, StackingClassifier"
      ],
      "metadata": {
        "id": "-4TdtAW-XfWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# список базовых моделей\n",
        "estimators = [(\"CatBoost\", Catboost_pipe),\n",
        "              ('SVC', pipe_svc),\n",
        "              ('Log_Reg', pipe_log)]\n",
        "\n",
        "# в качестве мета-модели будем использовать LogisticRegression\n",
        "meta_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(verbose=False, max_iter=200),\n",
        "    n_jobs=-1,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "stacking_classifier = meta_model\n",
        "stacking_classifier"
      ],
      "metadata": {
        "id": "sv1UKBObYGpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_st = stacking_classifier.predict(X_val)\n",
        "balanced_accuracy_score(y_true=y_val, y_pred=preds_st)\n",
        "accuracy_score(y_true=y_val, y_pred=preds_st)"
      ],
      "metadata": {
        "id": "1wscn5IoYJAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub = pd.read_csv('/content/drive/MyDrive/competitions/NLP2.0/data/base_submission_news.csv')\n",
        "preds = stacking_classifier.predict(test)\n",
        "df_sub['topic'] = preds\n",
        "df_sub.to_csv('try_5.csv', index=False)"
      ],
      "metadata": {
        "id": "CftO6u1bYMBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7\n",
        "После неудачи со стекингом, попробовал обучить ещё один бустинг."
      ],
      "metadata": {
        "id": "AD74qU_UYODU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "XGboost_pipe = Pipeline([('col_selector', CustomSelectColumns(['content_clean'], clean=True)),\n",
        "                     ('w2v_tfidf', TfidfEmbeddingVectorizer(model_w2v)),\n",
        "                    #  ('scaler', StandardScaler()),\n",
        "                     ('clf', XGBClassifier(random_state=RND_STATE,\n",
        "                            #  max_iter=10000,\n",
        "                            #  class_weight='balanced'\n",
        "                             ))])\n",
        "\n",
        "XGboost_pipe.fit(X_train, y_train,\n",
        "                #  clf__eval_set=(X_val, y_val)\n",
        "                 )\n",
        "\n",
        "print(classification_report(y_val, XGboost_pipe.predict(X_val)))"
      ],
      "metadata": {
        "id": "OCezvOy5Yahu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "опять же optuna использовалась. И одновременно добавлялись и добавлялись разные варианты данных."
      ],
      "metadata": {
        "id": "X00qrQ_IYggF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Где-то в этот момент я вспомнил, что в обсуждениях был вопрос о предобученных моделях. Но вообще я ответ понял, что можно использовать, но вообще было бы лучше на стандартных моделях работать.\n",
        "\n",
        "Использования catbosst + text_features - я вообще тоже считал неким читерством. И пытался выжать скорее из sklearn моделей что-то хорошее.\n",
        "\n",
        "+ важным нюансом оказалось, что я вообще пропустил, что была последняя лекция, на которой показывались варианты работы с bert.\n",
        "Т.е. когда в обсуждениях возник вопрос о предобученных моделях, я думал, что их вообще на курсе не показывали. И люди, просто с большим количеством изначальных знаний, которые были получены не на курсе, могли пользоваться своими знаниями.\n",
        "\n",
        "В общем, всё это было огромной ошибкой."
      ],
      "metadata": {
        "id": "fduYFgyMYoxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучал ещё несколько разных вариантов моделей из sklearn. multinomialNB, sgdclassifier ... Использовал стекинг. Всё это не давало результатов. + время конкурса уже закончилось. 4 числа."
      ],
      "metadata": {
        "id": "dBxw4kcNccD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8\n",
        "\n",
        "Где-то пятого числа я уже окончательно понял, что мне ничего не выжать из простых моделей. И вспомнил про предобученные модели. Я, в принципе, не очень знал, что это. Подумал, что видимо векторизаторы. И нашёл первый попавшийся"
      ],
      "metadata": {
        "id": "iH6kb6b9c7vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## navec"
      ],
      "metadata": {
        "id": "6z-akbl9c-I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install navec"
      ],
      "metadata": {
        "id": "YJhVfEh_c5JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.yandexcloud.net/natasha-navec/packs/navec_news_v1_1B_250K_300d_100q.tar"
      ],
      "metadata": {
        "id": "tGC8BTKJdZLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from navec import Navec\n",
        "\n",
        "path = '/content/navec_news_v1_1B_250K_300d_100q.tar'\n",
        "navec = Navec.load(path)"
      ],
      "metadata": {
        "id": "MwesYWGSda0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиение на обучающую и валидационную выборки\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RND_STATE)"
      ],
      "metadata": {
        "id": "ZCYECtG9deZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_navec_vector(sentence):\n",
        "    Sum = 0\n",
        "    Count = 0\n",
        "\n",
        "    words = sentence.split()\n",
        "\n",
        "    for w in words:\n",
        "        if w in navec:\n",
        "            Sum += navec[w]\n",
        "            Count += 1\n",
        "\n",
        "    return Sum / Count"
      ],
      "metadata": {
        "id": "T12lTIvIdgCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NewCols = ['col'+str(i) for i in range(300)]"
      ],
      "metadata": {
        "id": "MPCA-Of4ddFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_navec = X_train.copy()\n",
        "X_test_navec = X_val.copy()\n",
        "\n",
        "X_train_navec['vectors'] = X_train['content_clean'].map(get_mean_navec_vector)\n",
        "X_test_navec['vectors'] = X_val['content_clean'].map(get_mean_navec_vector)"
      ],
      "metadata": {
        "id": "mb4594__diiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_navec[NewCols] = pd.DataFrame(X_train_navec['vectors'].tolist(), index= X_train_navec.index)\n",
        "X_test_navec[NewCols] = pd.DataFrame(X_test_navec['vectors'].tolist(), index= X_test_navec.index)"
      ],
      "metadata": {
        "id": "9h04S_d7dkGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_navec.drop(['content','content_clean','vectors'], axis=1, inplace=True)\n",
        "X_test_navec.drop(['content','content_clean', 'vectors'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "WHwS6lyOdnDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавление просто этих дополнительных векторов в датасет только ухудшило ситуацию. Но я не получал окончательное вариант усреднения для предложений, а просто подавал вот полученные вектора (length = 300) в модель."
      ],
      "metadata": {
        "id": "uJGv9DS5eAE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я подумал, что надо попоробовать векторизаторы из !pip install transformers -q. И даже запустил векторизацию, впрочем всё равно это было не совсем так как было показано на занятии. Одним из важных отличий -подавал не батчами. И я около 5 часов ждал векторизации хотя бы данных train, но не дождался и выключил.\n",
        "\n",
        "Уже оставалось очень мало времени, как я предполагал. И ещё верил, что может быть с navec векторами и стекингом у меня получится выжать что-то побольше. + считал, что использовать предобученные векторизаторы скорее не приветствуется."
      ],
      "metadata": {
        "id": "SvPthSAeeIhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поэтому окончательный вариант был большое количесто стекинга https://github.com/nedokormysh/kaggle_nlp2.0/blob/main/notebooks/NLP2_classification_full_all_new_last.ipynb"
      ],
      "metadata": {
        "id": "zOgkMNUIeP0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# список базовых моделей\n",
        "estimators = [(\"CatBoost\", Catboost_pipe),\n",
        "              ('SVC', pipe_svc),\n",
        "              ('XGboost_pipe', XGboost_pipe),\n",
        "              ('pipe_SGD', pipe_SGD),\n",
        "              ('Log_Reg', pipe_log)]\n",
        "\n",
        "# в качестве мета-модели будем использовать LogisticRegression\n",
        "meta_model = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(verbose=False, max_iter=200),\n",
        "    # n_jobs=-1,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "stacking_classifier = meta_model\n",
        "stacking_classifier"
      ],
      "metadata": {
        "id": "va4fuk5Nfo5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пытался сблендить результаты разных моделей -выбрать самый часто встречающийся ответ - не дало результатов."
      ],
      "metadata": {
        "id": "7Ml-ovqIfqVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В окончательное решение попала первая модель с хорошим скором catboost на text_features и стекинг c navec векторами. (стекинг по локальному скору и по скору на public был достаточно плох, но была надежда, что возможно некая генерализация решения будет - опять ошибочный вариант.)"
      ],
      "metadata": {
        "id": "a-O1_qByfxeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собственно, зная, что соревнование окончится 8-го - можно было бы ещё раз попробовать обучить catboost, но уже на bert векторах. Но я подумал, что просто уже не успею ничего сделать. Немного увеличение сроков для меня играло скорее в минус. Т.е. для меня это было скорее неожиданностью."
      ],
      "metadata": {
        "id": "FsDaIqpTgeZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучшим решением остался catboost, обученный только на части данных."
      ],
      "metadata": {
        "id": "lplxl9Yng8c6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Насколько я понял, то топовые решения использовали bert. И я увидел, что bert показывался на занятиях только 6-го числа."
      ],
      "metadata": {
        "id": "N439iwwQhHmB"
      }
    }
  ]
}